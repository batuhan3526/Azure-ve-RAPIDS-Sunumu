{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd25c9fd-74cb-49d5-afa5-d60d06367c88",
   "metadata": {},
   "source": [
    "# **Multi-GPU Numerical Computing: Dask cuDF + Numba CUDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea8ff6-7815-488f-81e2-cffc5b364071",
   "metadata": {},
   "source": [
    "## **Prerequisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816379b-137b-4faf-9180-d22a40d765ec",
   "metadata": {},
   "source": [
    "This tutorial assume proficiency in Python and the following libraries:\n",
    "\n",
    "* pandas/cuDF\n",
    "* NumPy/CuPy\n",
    "* Numba\n",
    "\n",
    "Demo System - Benchmarking was performed on a DGX Station A100 320GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654c2c3-102f-4259-a530-ad0d83ee515a",
   "metadata": {},
   "source": [
    "## **Why Dask cuDF + Numba**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616609a-3121-4dde-a442-f8ff2f10fb9d",
   "metadata": {},
   "source": [
    "[Numba CUDA Python](https://numba.readthedocs.io/en/stable/cuda/index.html) has been used to GPU accelerate code without leaving Python. This is extremely compelling for those performing rapid prototyping or maintain a desire to stay in Python. Most examples and applications of Numba are single GPU. This notebook demonstrates achiving higher kernel proformance on a compute intensive workload using multiple GPUs in a local cluster managed by Dask.\n",
    "\n",
    "**When to consider this programming pattern:**\n",
    "1. You have a larger than memory data problem\n",
    "2. You are fully saturating a single GPUs\n",
    "3. Streaming data needs to bel loaded and rapidly processed by an expensive kernel\n",
    "4. You do not want to develop your own job scheduling software\n",
    "\n",
    "**Note:**\n",
    "1. Sending data between devices implies an I/O penalty and overhead -- performance improvements will be most pronounced if already saturating a single GPU (for smaller problems, overheads could dominate performance)\n",
    "2. Dask cuDF will be more difficult to use with many n-dimensional array problems -- also consider Dask Arrays and cuNumeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514172af-1508-4d4f-b502-c27d981b7e11",
   "metadata": {},
   "source": [
    "## **Problem Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e1ff3-4fe5-4528-85be-8d96b9eaa14a",
   "metadata": {},
   "source": [
    "We will explore a common nearest neighbors problem in ND-array computing to explore these programming paradigms. Although there are some more efficient techniques to solve this problem -- we add an interesting constraint, our reference points change after each calculation, making it more difficult to apply precomputed indexing strategies. As a result, we focus our attention on the brute force technique. The brute force technique is easier to grasp and increases the amount of calculations and comparisons we need to do, and thereby, its arithmetic intensity will challenge each method. We'll leverage a data generation script to simulate geospatial points in radians and use the haversine great circle distance as our distance metric. This is a popular technique used to calculate the distance between two points on earth.\n",
    "\n",
    "<center><img src=\"./media/haversine-graphic.png\" alt=\"RAPIDS Logo\" style=\"width: 150;\"/></center></br>\n",
    "\n",
    "The graphic below illustrates the dynamic nature of the problem we are solving and implies the need for compute efficient as each timestep approaches zero.\n",
    "\n",
    "<center><img src=\"./media/DynamicDecisionBoundaries.png\" alt=\"RAPIDS Logo\" style=\"width: 1000;\"/></center>\n",
    "\n",
    "In this notebook, we will evaluate running our Numba CUDA Kernel (from our single cpu/gpu notebook) using all the GPUs on our demo system on a problem scaled up by 2048x - 8.8T\n",
    "\n",
    "**Spoiler Alert -- This Multi-GPU techniques out perform the Multi-CPU technique by orders of magnitude.**\n",
    "\n",
    "<center><img src=\"./media/AllScaledCpuGpuPerfTable.png\" alt=\"RAPIDS Logo\" style=\"width: 1000;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44717e7e-8048-4bbd-abfb-fe627e38f6ed",
   "metadata": {},
   "source": [
    " # **Multi-GPU Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11688d32-5a0f-49bc-9860-2f8fc06ccec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import cupy as cp\n",
    "from numba import cuda\n",
    "\n",
    "from src.solvers import (block_min_reduce,\n",
    "                         global_min_reduce)\n",
    "\n",
    "from src.simulator import generate_geos\n",
    "from src.utils import check_accuracy\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a0cfd-86d5-4110-bb43-aa1a29da5d4a",
   "metadata": {},
   "source": [
    "Define constants for the size of our experiment and evaluation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa369f-2052-4ec4-b389-a3b2f917f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS, N_REF = 2**27, 2**16 # single processor experiment\n",
    "N_OBS_VAL, N_REF_VAL = 500, 200 # check accuracy\n",
    "print(\"Problem Size (N_OBS * N_REF): {:.2f}T\".format(N_OBS * N_REF * 1e-12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f303f-53b8-4c7a-bc8b-abee03689c30",
   "metadata": {},
   "source": [
    "## **Start Dask CUDA Cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f7880-7766-4555-97d3-ff57150abe1c",
   "metadata": {},
   "source": [
    "With a few lines of code spin up a local dask cluster of GPUs that can be scheduled to complete our workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db16ce6-9539-4d42-905b-6cfde567b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b90050-8068-4245-b494-a74073a5ab2d",
   "metadata": {},
   "source": [
    "## **Define ```map_partitions``` function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42a50c-ee68-4030-b5d6-38a8ea71ca10",
   "metadata": {},
   "source": [
    "The ```_get_nearest_part``` function will perform our double Numba CUDA kernal launch pattern outlined in the single gpu/cpu notebook.  Here we are launching these kernel to execute on chunks of our data partitioned by Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f305bf-2160-4612-a6a0-668c1926a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nearest_part(part_df, coord2=None, block_idx=None, block_dist=None):\n",
    "    \n",
    "    coord1 = part_df[[\"LAT_RAD\", \"LON_RAD\"]].to_cupy()\n",
    "    coord2 = coord2.to_cupy()\n",
    "\n",
    "    block_idx_mat = cp.empty(\n",
    "        (coord1.shape[0], 32), \n",
    "        dtype=np.uint32\n",
    "    )\n",
    "    \n",
    "    block_dist_mat = cp.empty(\n",
    "        (coord1.shape[0], 32),\n",
    "        dtype=np.float32\n",
    "    )        \n",
    "    \n",
    "    out_idx = cp.empty(\n",
    "        (coord1.shape[0]), \n",
    "        dtype=np.uint32\n",
    "    )\n",
    "    \n",
    "    out_dist = cp.empty(\n",
    "        (coord1.shape[0]), \n",
    "        dtype=np.float32\n",
    "    )    \n",
    "    \n",
    "    bpg = 32, 108\n",
    "    tpb = 32, 16    \n",
    "    \n",
    "    block_min_reduce[bpg, tpb](\n",
    "        coord2, \n",
    "        coord1, \n",
    "        block_idx_mat,\n",
    "        block_dist_mat\n",
    "    )   \n",
    "\n",
    "    bpg = (1, 108*20)\n",
    "    tpb = (32, 16)    \n",
    "    \n",
    "    global_min_reduce[bpg, tpb](\n",
    "        block_dist_mat, \n",
    "        block_idx_mat, \n",
    "        out_dist, \n",
    "        out_idx\n",
    "    )      \n",
    "    \n",
    "    cuda.synchronize()\n",
    "    \n",
    "    part_df[\"out_idx\"] = out_idx\n",
    "    part_df[\"out_dist\"] = out_dist\n",
    "            \n",
    "    return (part_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3524a-63d5-47c5-9be6-7e32c1f92ad3",
   "metadata": {},
   "source": [
    "## **Define Multi-GPU Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b512d9-50d2-4ae9-b890-76d417d4d905",
   "metadata": {},
   "source": [
    "Define a function that will distribute our dataset across the local CUDA cluster and map our ```_get_nearest_part``` function to each data partition. Final results are returned in a cuDF DataFrame residing on the default GPU.\n",
    "\n",
    "Note - we did not have to develop our own job scheduling mechanism, Dask handles this for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d99b57-44ee-43ec-bebb-5653d0e2256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest(d_obs, d_ref):\n",
    "\n",
    "    gdf_ref = cudf.DataFrame()\n",
    "    gdf_obs = cudf.DataFrame()\n",
    "    \n",
    "    gdf_ref[\"LAT_RAD\"] = d_ref[:,0]\n",
    "    gdf_ref[\"LON_RAD\"] = d_ref[:,1]\n",
    "    gdf_obs[\"LAT_RAD\"] = d_obs[:,0]\n",
    "    gdf_obs[\"LON_RAD\"] = d_obs[:,1]\n",
    "    \n",
    "    ddf = dask_cudf.from_cudf(gdf_obs, npartitions=4)\n",
    "    \n",
    "    gdf_result = ddf.map_partitions(\n",
    "        _get_nearest_part, \n",
    "        coord2=gdf_ref,\n",
    "    ).compute()\n",
    "    \n",
    "    return (cp.asarray(gdf_result[\"out_idx\"]),\n",
    "            cp.asarray(gdf_result[\"out_dist\"]))\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ccc343-689b-4fe8-9d1d-bd71ed34aea9",
   "metadata": {},
   "source": [
    "## **Generate Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ab12b-bf5c-492d-9490-9a5b4e3f97fa",
   "metadata": {},
   "source": [
    "Let's generate a scaled up synthetic dataset and validation dataset for our work today using an included utility function. These datasets represent the following:\n",
    "\n",
    "* ```d_obs``` contains ```N_OBS``` geospatial observations in radians on the GPU, used for our full scale benchmark\n",
    "* ```d_ref``` contains ```N_REF``` geospatial reference points in radians on the GPU, used for our full scale benchmark\n",
    "* ```d_obs_val``` contains ```N_OBS_VAL``` geospatial observations points in radians on the GPU, used to validate accuracy\n",
    "* ```d_ref_val``` contains ```N_REF_VAL``` geospatial reference points in radians on the GPU, used to validate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec45c02-9eab-4496-9e0d-bb238c664ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ref = generate_geos(N_REF, random_state=1)\n",
    "d_obs = generate_geos(N_OBS, random_state=2)\n",
    "\n",
    "d_ref_val = generate_geos(N_REF_VAL, random_state=1)\n",
    "d_obs_val = generate_geos(N_OBS_VAL, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b56db-4725-4640-a481-9f741faf3939",
   "metadata": {},
   "source": [
    "## **Validate Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a9de3-3d53-4d29-9002-893d791aaa8b",
   "metadata": {},
   "source": [
    "Verify our multi-GPU implementation is producing the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05a34f-59ad-438b-b60f-8909acb37284",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_val_idx, d_val_dist = get_nearest(\n",
    "    d_obs_val, \n",
    "    d_ref_val\n",
    ")\n",
    "\n",
    "print(\"Accuracy - Dask Numba CUDA Multi-GPU:\", \n",
    "      check_accuracy(\n",
    "          d_obs_val, \n",
    "          d_ref_val,\n",
    "          d_val_idx, \n",
    "          d_val_dist)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835670ee-9a7f-428c-b778-cf96c30f61d5",
   "metadata": {},
   "source": [
    "## **Benchmark Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab61e0c-39b0-491b-b95e-cfb9f8bcf789",
   "metadata": {},
   "source": [
    "We observe our kernel completes in roughly 14.7ms on our demo system, ~511x faster than the multi-CPU alternative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49284b34-0376-4a8d-9d6f-661361780de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "d_val_idx, d_val_dist = get_nearest(\n",
    "    d_obs, \n",
    "    d_ref\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b9a888-3c48-4f1b-9fdf-18dcf738c16a",
   "metadata": {},
   "source": [
    "# **Summarize Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee54ea3-0e18-4e22-a0e6-b6361018ee88",
   "metadata": {},
   "source": [
    "In summary, we observe our multi-GPU technique solves our scaled up problem orders of magnitude faster than the parallel CPU alternative. This implementation required less developer effort than the tailor-made solution implementation, but achieved slightly lower performance for this use case. We also acknowledge this might be less appropriate many n-dimensional array problems (e.g. non-DataFrame operations).\n",
    "\n",
    "<img src=\"./media/MultiScaledCpuGpuPerfTable.png\" alt=\"RAPIDS Logo\" style=\"width: 150;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f9abb-e102-4ff6-b70c-613502eac0e5",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div align=\"left\"><h2><b>Please Restart the Kernel<b></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b8d9a-5909-441d-ab00-38640a584480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
