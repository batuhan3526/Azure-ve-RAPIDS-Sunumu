{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dead3620-674b-4519-adac-1e9dd0d534a9",
   "metadata": {},
   "source": [
    "# **Multi-GPU Numerical Computing: Threading + RMM + Numba CUDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a2b01-a40f-4140-b597-73f60e4afbe5",
   "metadata": {},
   "source": [
    "## **Prerequisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18117a6c-b267-4611-884e-8c5d669c4534",
   "metadata": {},
   "source": [
    "This tutorial assume proficiency in Python and the following libraries:\n",
    "\n",
    "* Threading\n",
    "* Numba\n",
    "* NumPy\n",
    "\n",
    "Demo System - Benchmarking was performed on a DGX Station A100 320GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5deb0a-2918-478a-b60e-ef8ac0e9ffd7",
   "metadata": {},
   "source": [
    "## **Why Multi-GPU Numba CUDA?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a01778-262f-49b6-9aae-aa9e660795d8",
   "metadata": {},
   "source": [
    "[Numba CUDA Python](https://numba.readthedocs.io/en/stable/cuda/index.html) has been used to GPU accelerate code without leaving Python. This is extremely compelling for those performing rapid prototyping or maintain a desire to stay in Python. Most examples and applications of Numba are single GPU. This notebook demonstrates achieving higher Numba CUDA kernel performance on a compute intensive workload using multiple GPUs and the [RAPIDS Memory Manager (RMM)](https://github.com/rapidsai/rmm).\n",
    "\n",
    "**When to consider this programming pattern:**\n",
    "1. Kernel needs to run faster and is saturating a single GPU\n",
    "2. Two independent kernels need to run simultaneously without competing for resources\n",
    "3. Streaming data needs to be loaded into GPU and processed rapidly by an expensive kernel\n",
    "\n",
    "**Note:**\n",
    "1. Sending data between devices implies an I/O penalty and overhead -- performance improvements will be most pronounced if already saturating a single GPU (for smaller problems, overheads could dominate performance)\n",
    "2. Use managed memory allocation strategy to share data between multiple GPUs\n",
    "3. Data can also be shared between GPUs using use Numba's APIs page-locked ([pinned](https://numba.pydata.org/numba-doc/dev/cuda-reference/memory.html#numba.cuda.pinned_array) or [mapped](https://numba.pydata.org/numba-doc/dev/cuda-reference/memory.html#numba.cuda.mapped_array), but traversed the PCIe bus (not shown here)\n",
    "4. Only on Numba CUDA context can be active at a time per CPU thread\n",
    "\n",
    "*Inspired by this example:\n",
    "https://github.com/ContinuumIO/numbapro-examples/tree/master/multigpu*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9406ff-1a39-4379-8518-0670239edd29",
   "metadata": {},
   "source": [
    "## **Problem Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61a63b-580c-4d1c-9143-81a9cdaebd1f",
   "metadata": {},
   "source": [
    "In this notebook, we leverage a proxy geospatial nearest neighbor problem to guide us through an evaluation of a tailor-made multi-GPU technique using Python threading + RMM + Numba CUDA. In this use case, we aim to resolve geospatial observations to their nearest reference points with an added complexity. Our complication adds dynamics to the problem allowing each reference point to move and the set of observations to change on a reoccurring basis. These complexities imply a need to recompute each nearest neighbor at each timestep -- emphasizing the need for high performance techiques. \n",
    "\n",
    "Because of its simplicity and arithmetic intensity, we focus our attention on the brute force nearest neighbor technique using the haversine great circle distance formula as our distance metric. This is a popular formula used to calculate the distance between two points on earth.\n",
    "\n",
    "<center><a href=\"https://en.wikipedia.org/wiki/Haversine_formula\"><img src=\"./media/haversine-graphic.png\" alt=\"Haversine\" style=\"width: 150;\"></a></center></br>\n",
    "\n",
    "The graphic below illustrates the dynamic nature of our problem. From left to write, we can observe the dynamics of the system at each timestep -- with colored regions representing nearest neighbor decision boundaries for each reference point and points representing observations.\n",
    "\n",
    "<center><img src=\"./media/DynamicDecisionBoundaries.png\" alt=\"Visualization\" style=\"width: 1000;\"/></center>\n",
    "\n",
    "In this notebook, we will use all available GPUs (4xA100) on a problem scaled up by 2048x - 8.8T\n",
    "\n",
    "**Spoiler Alert -- This GPU technique out performs the parallel CPU example by almost 600x.**\n",
    "\n",
    "<center><img src=\"./media/AllScaledCpuGpuPerfTable.png\" alt=\"PerfTable\" style=\"width: 1000;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80383064-ab14-4e61-818b-bea59df69b39",
   "metadata": {},
   "source": [
    " # **Multi-GPU Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade5b83-95ca-4a59-8298-ddd0213e725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import cupy as cp\n",
    "import numba as nb\n",
    "import rmm\n",
    "import cudf\n",
    "\n",
    "from src.solvers import (block_min_reduce,\n",
    "                         global_min_reduce)\n",
    "\n",
    "from src.simulator import generate_geos\n",
    "from src.utils import check_accuracy\n",
    "import math\n",
    "import threading\n",
    "import queue\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30a856-6121-4a14-bd71-9ad2d3839408",
   "metadata": {},
   "source": [
    "Define constants for the size of our experiment and evaluation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad3ab5-2671-4d15-bac9-9a497e7fa0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS, N_REF = 2**27, 2**16 # single processor experiment\n",
    "N_OBS_VAL, N_REF_VAL = 500, 200 # check accuracy\n",
    "print(\"Problem Size (N_OBS * N_REF): {:.2f}T\".format(N_OBS * N_REF * 1e-12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0213ae-139a-40dc-99dc-4f0e0f3ab0c5",
   "metadata": {},
   "source": [
    "## **RAPIDS Memory Manager**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04bffd3-5994-48fc-ade1-45083bc784d4",
   "metadata": {},
   "source": [
    "Use RMM to define a managed memory GPU allocation strategy, making it easy handle multi-GPU communications over NVLink/NVSwitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f48d4b-85fd-4e17-809f-19b429ad470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use managed memory for allocations\n",
    "cuda.set_memory_manager(rmm.RMMNumbaManager)\n",
    "\n",
    "rmm.mr.set_current_device_resource(\n",
    "    rmm.mr.ManagedMemoryResource())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37fd097-cbcf-4e64-8f68-9b2b895313b9",
   "metadata": {},
   "source": [
    "## **Define Thread Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc703b-78f5-40c2-8384-88baa7b88eec",
   "metadata": {},
   "source": [
    "The ```_get_nearest_multi``` thread function will perform a double Numba CUDA kernel launch pattern outlined in the single gpu/cpu notebook. Here we launch kernels to operate on partitions of data defined in a job configuration object ```q```.  Each thread has a unique CUDA context defined by GPU index position ```cid```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f416707-dc22-4046-a23b-5d4eeae134fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_nearest_multi(q, cid):\n",
    "        \n",
    "    cuda.select_device(cid) # bind device to thread\n",
    "        \n",
    "    while(q.unfinished_tasks > 0):\n",
    "\n",
    "        job = q.get()\n",
    "                \n",
    "        d_ref = cuda.to_device(\n",
    "            job[\"d_m_ref\"]\n",
    "        )\n",
    "        \n",
    "        d_obs = cuda.to_device(\n",
    "            job[\"d_m_obs\"][job[\"start\"]:job[\"end\"]]\n",
    "        )\n",
    "        \n",
    "        d_block_idx = cuda.device_array(\n",
    "            (job[\"end\"] - job[\"start\"], 32), \n",
    "            dtype=np.uint32)\n",
    "        \n",
    "        d_block_dist = cuda.device_array(\n",
    "            (job[\"end\"] - job[\"start\"], 32), \n",
    "            dtype=np.float32)        \n",
    "                \n",
    "        bpg = 32, 108\n",
    "        tpb = 32, 16\n",
    "\n",
    "        block_min_reduce[bpg, tpb](\n",
    "            d_ref,\n",
    "            d_obs,\n",
    "            d_block_idx,\n",
    "            d_block_dist           \n",
    "        )\n",
    "        \n",
    "        bpg = (1, 108*20)\n",
    "        tpb = (32, 16)\n",
    "\n",
    "        global_min_reduce[bpg, tpb](\n",
    "            d_block_dist,\n",
    "            d_block_idx,\n",
    "            job[\"d_m_out_dist\"][job[\"start\"]:job[\"end\"]],\n",
    "            job[\"d_m_out_idx\"][job[\"start\"]:job[\"end\"]]\n",
    "        )        \n",
    "\n",
    "        cuda.synchronize()\n",
    "\n",
    "        q.task_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf35983-c1bb-4ab3-94ea-fc4092ffe925",
   "metadata": {},
   "source": [
    "## **Define Multi-GPU Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafaa426-177b-4e03-a661-34a5bee403dc",
   "metadata": {},
   "source": [
    "Based on available GPU configuration, build up a queue ```queues``` of jobs (kernel launches on partitions) to be scheduled in each CPU thread. Our thread function ```_get_nearest_multi``` grabs jobs from this queue and performs the work in its own CPU thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e194db1-48e3-4c55-9068-f629da453f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest(\n",
    "    obs_points, ref_points,\n",
    "    batch_size=\"auto\",\n",
    "    multigpu=True,\n",
    "    n_gpus=\"auto\"):\n",
    "    \n",
    "    out_idx = cuda.device_array(\n",
    "        (obs_points.shape[0]),\n",
    "        dtype=np.uint32)\n",
    "    \n",
    "    out_dist = cuda.device_array(\n",
    "        (obs_points.shape[0]),\n",
    "        dtype=np.float32)\n",
    "    \n",
    "    if not multigpu:\n",
    "        \n",
    "        d_block_idx = cuda.device_array(\n",
    "            (out_idx.shape[0], 32), \n",
    "            dtype=np.uint32)\n",
    "        \n",
    "        d_block_dist = cuda.device_array(\n",
    "            (out_idx.shape[0], 32), \n",
    "            dtype=np.float32)           \n",
    "        \n",
    "        bpg = 32, 108\n",
    "        tpb = 32, 16\n",
    "              \n",
    "        block_min_reduce[bpg, tpb](\n",
    "            ref_points, \n",
    "            obs_points, \n",
    "            d_block_idx,\n",
    "            d_block_dist\n",
    "        )   \n",
    "        \n",
    "        bpg = (1, 108*20)\n",
    "        tpb = (32, 16)        \n",
    "\n",
    "        global_min_reduce[bpg, tpb](\n",
    "            d_block_dist, \n",
    "            d_block_idx, \n",
    "            out_dist, \n",
    "            out_idx\n",
    "    )   \n",
    "        \n",
    "        cuda.synchronize()\n",
    "        \n",
    "        return out_idx, out_dist\n",
    "\n",
    "    if n_gpus == \"auto\":\n",
    "        n_gpus = len(cuda.list_devices())\n",
    "        \n",
    "    size = obs_points.shape[0]\n",
    "        \n",
    "    if batch_size == 'auto':\n",
    "        batch_size = size/(n_gpus)\n",
    "        \n",
    "    batch_size = int(batch_size)\n",
    "\n",
    "    n_jobs = int(size / min(batch_size, size))\n",
    "        \n",
    "    queues = [queue.Queue() for i in range(n_gpus)]\n",
    "    \n",
    "    qid = 0\n",
    "    \n",
    "    for j in range(n_jobs):\n",
    "        \n",
    "        if qid >= len(queues):\n",
    "            qid = 0\n",
    "\n",
    "        job = {}\n",
    "        \n",
    "        start = j * batch_size\n",
    "        \n",
    "        if j == (n_jobs - 1):\n",
    "            end = size\n",
    "        else:\n",
    "            end = (j + 1) * batch_size\n",
    "        \n",
    "        job[\"start\"] = start\n",
    "        job[\"end\"] = end\n",
    "        job[\"d_m_ref\"] = ref_points\n",
    "        job[\"d_m_obs\"] = obs_points\n",
    "        job[\"d_m_out_idx\"] = out_idx\n",
    "        job[\"d_m_out_dist\"] = out_dist        \n",
    "        \n",
    "        queues[qid].put(job)\n",
    "        \n",
    "        qid += 1\n",
    "    \n",
    "    workers = []\n",
    "    \n",
    "    for qid in range(len(queues)):\n",
    "        \n",
    "        w = threading.Thread(\n",
    "            target=_get_nearest_multi, \n",
    "            args=[queues[qid], \n",
    "                  qid])\n",
    "        \n",
    "        w.start()\n",
    "        workers.append(w)\n",
    "        \n",
    "    for w in workers:\n",
    "        w.join()     \n",
    "        \n",
    "    return out_idx, out_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5917802-7803-4c9d-8849-82182e0befdd",
   "metadata": {},
   "source": [
    "## **Generate Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55a933-c0b9-426a-a591-198fdc66f43b",
   "metadata": {},
   "source": [
    "Let's generate a scaled up synthetic dataset and validation dataset for our work today using an included utility function. These datasets represent the following:\n",
    "\n",
    "* ```d_obs``` contains ```N_OBS``` geospatial obserations in radians on the GPU, used for our full scale benchmark\n",
    "* ```d_ref``` contains ```N_REF``` geospatial reference points in radians on the GPU, used for our full scale benchmark\n",
    "* ```d_obs_val``` contains ```N_OBS_VAL``` observations in radians on the GPU, used to validate accuracy\n",
    "* ```d_ref_val``` contains ```N_REF_VAL``` geospatial reference points in radians on the GPU, used to validate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee683e-ee80-4bc5-8f08-e243643d74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ref = generate_geos(N_REF, random_state=1)\n",
    "d_obs = generate_geos(N_OBS, random_state=2)\n",
    "\n",
    "d_ref_val = generate_geos(N_REF_VAL, random_state=1)\n",
    "d_obs_val = generate_geos(N_OBS_VAL, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9549bdb-9a78-4d48-8b43-1ad95b294904",
   "metadata": {},
   "source": [
    "## **Validate Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e2ad1-a04e-4753-9c33-627763d4a275",
   "metadata": {},
   "source": [
    "Verify our multi-GPU method is producing the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b35ec-2154-45af-ba5e-5bc516dea092",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_idx_val, d_dist_val = get_nearest(d_obs_val, d_ref_val, \n",
    "            batch_size='auto', \n",
    "            multigpu=True, n_gpus=\"auto\")\n",
    "\n",
    "print(\"Accuracy - Threading RMM Numba CUDA Multi-GPU:\", \n",
    "      check_accuracy(\n",
    "          d_obs_val, \n",
    "          d_ref_val,\n",
    "          d_idx_val, \n",
    "          d_dist_val)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37cd75-cc29-468b-bde2-1a0bd4f7bd62",
   "metadata": {},
   "source": [
    "## **Benchmark Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375a8ab-d9fb-470f-8e9b-20ebeb26b5b3",
   "metadata": {},
   "source": [
    "Our kernel completes in 12.7s on the demo system, ~584x faster than the parallel CPU alternative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0bc85-a7d0-476e-bc95-1165085fa4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "d_idx, d_dist = get_nearest(\n",
    "    d_obs, d_ref, \n",
    "    batch_size='auto',\n",
    "    multigpu=True, \n",
    "    n_gpus=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc60b8-3f11-4c23-8c1a-e7fcd0769b90",
   "metadata": {},
   "source": [
    "# **Summarize Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5875f-cdf9-4db4-8476-6edc639b3bf3",
   "metadata": {},
   "source": [
    "In summary, we observe our multi-GPU technique solves our scaled up problem orders of magnitude faster than the parallel CPU alternative. This implementation required more developer effort than the ```dask_cudf``` implementation, but achieved slightly higher performance for this use case. We also benefit from more control over problems with n-dimensional arrays.\n",
    "\n",
    "<img src=\"./media/MultiScaledCpuGpuPerfTable.png\" alt=\"PerfTable\" style=\"width: 150;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c12d4a-7906-4096-8eb3-e277aa2a099a",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div align=\"left\"><h2><b>Please Restart the Kernel<b></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41fca4a-f726-4e57-836c-820fa57416a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
